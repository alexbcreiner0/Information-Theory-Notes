\contentsline {part}{I\hspace {1em}Shannon Information Theory}{2}{part.1}%
\contentsline {section}{\numberline {1}Entropy}{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}Defining Entropy}{3}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Conditional Entropy and Mutual Information}{6}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}General Theory of Entropy}{8}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Fano's Inequality}{14}{subsection.1.4}%
\contentsline {section}{\numberline {2}The Asymptotic Equipartition Property}{15}{section.2}%
\contentsline {section}{\numberline {3}Entropy Rates of Stochastic Processes}{18}{section.3}%
\contentsline {subsection}{\numberline {3.1}The Second Law of Thermodynamics}{20}{subsection.3.1}%
\contentsline {section}{\numberline {4}Data Compression}{23}{section.4}%
\contentsline {section}{\numberline {5}Data Transmission}{24}{section.5}%
\contentsline {subsection}{\numberline {5.1}Channel Capacity}{24}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Regulation and Ashby's Law of Requisite Variety}{36}{subsection.5.2}%
\contentsline {part}{II\hspace {1em}Algorithmic Information Theory}{38}{part.2}%
