\subsection{Regulation and Ashby's Law of Requisite Variety}
	Our setup is the following. First, we have a system of some sort.  
What we are calling a system is whatever our regulator is seeking to \textbf{stabilize}, and the relevant states of the system are determined by the aspects of this system which we care about. We denote the state of this system $E$, and view it as a discrete random variable. It is important to see this system concretely, as the thing itself, but it is equally important to see it as an information source, or more specifically a \emph{variety generator}. Our second information source generating variety is the external environment itself. This information source produces disturbances which will alter or disrupt the stability of the system. We thus denote this source of information $D$. Lastly, we have the regulator itself. We will refer to the regulator as a machine, even if it isn't. This regulator is an information source just like the other two; the information created is the action taken moment to moment. We denote this information source $R$. \par 
	The goal of regulation is in some sense the opposite of the goal of a channel. With a channel, we are seeking to maximize the mutual information between the information generated by a source and the information generated at the opposite side of the channel. With a regulator on the other hand, we are seeking to minimize the mutual information. If a regulator does it's job properly, it completely destroys any information about the disturbance $D$. A few assumptions about the control systems we are interested in are necessary. First, our regulator is to be seen as a machine. This is to say that it's action is a deterministic function of the disturbance. Because of this, the conditional uncertainty of the regulator given the disturbance is zero. That is to say, we will assume
	\begin{align}
		H(R|D) = 0
	\end{align}
Next we consider the conditional uncertainty of the regulator given the disturbance. We definitely don't, in general, have that $H(D|R) = 0$ even if the machine is deterministic. This is because the regulator might want to take the same action for multiple different kinds of disturbances. In other words, if $R = f(D)$, then the function $f$ need not be injective. Nonetheless, we are looking to derive conditions on the regulator which are necessary to achieve stability. We need to be careful not to hide anything from ourselves, and this requires putting a bound on the sophistication of the actions that our regulator can take. In particular, we need to assume that if we fix a particular action of the regulator, then any new disturbance will require the regulator to change it's action. If this weren't the case for some action of the machine, then that would mean that this action of the machine is safeguarding the system from multiple possible disturbances at once, and this would mean we are smuggling extra variety into our regulator without counting it. In order to properly keep track of the variety therefore, we add the condition that if we fix the action of the regulator, then the number of possible effects should therefore be at least as large as the number of possible disturbances. (Put another way, if we stop allowing the regulator to regulate, it should be as if the regulator isn't there in the first place.) Expressed in terms of entropy we have the assumption:
 \begin{align}
	H(E|R) \geq H(D|R)
\end{align}
Now, using the chain rule for joint entropy we can express $H(D,R)$ in two different ways:
\begin{align}
	H(D,R) = H(D)+H(R|D) = H(R)+H(D|R)
\end{align}
Now $H(R|D) = 0$ as we said. And $H(D|R) \leq H(E|R)$. Therefore
\begin{align}
	H(D) &= H(R)+H(D|R) \\ &\leq H(R)+H(E|R) \\ &= H(E,R) \\ &\leq H(E)+H(R)
\end{align}
So then we have 
\begin{align}
	H(D) \leq H(E)+H(R)
\end{align}
Rearranging the terms then gives 
\begin{align}
	H(E) \geq H(D)-H(R)
\end{align}
Recall that our goal here is \emph{stability}. This is \emph{all} that we want, and therefore we should have as little uncertainty about $E$ as possible, regardless of the disturbance $D$. If the goal is therefore to minimize $E$ and we have no control over $D$, then there is only one way to do it: our regulator must have sufficient variety generating potential to absorb the variety created by $D$. This is Ashby's law:
\begin{theorem}[Ashby's Law of Requisite Variety]
	A regulator seeking to fully stabilize a system it is overseeing must have at least as much variety as that of the possible disturbances which could disrupt that stability. Moreover achieving partial stability amounts to approaching coming close to the variety of $D$.
\end{theorem}
When transmitting data across a channel, we are seeking to maximize the mutual information between the two sides of the channel. There is a third party in this process which seeks to disrupt this transmission - a noise generator, seeking to destroy information, which we are working against when designing channels. In thinking about regulation, we are playing the role of the noise generator, seeking to destroy information before it gets anywhere. In other words, the variety seeks to disrupt this information flow, and \emph{minimize} the mutual information between the two sides of the channel. Ashby's Law gives us a minimal condition for accomplishing this task. It tells us that the variety of the noise generator must match the variety of the source if it is to have any hope of success. \par 
Bringing in the second law of thermodynamics completes the story. We are all machines, seeking to maximize our own local stability, i.e. reduce our local entropy. However, we are all part of a larger overall system, and the second law of thermodynamics tells us that entropy is non-decreasing. If I bring my own local entropy down from $H$ to $H-\epsilon$, then the entropy of the surrounding system which excludes me must increase by that much. In the pursuit of reducing entropy local, I serve the cosmic purpose of increasing entropy, globally. In other words, all machines are variety attenuators relative to themselves, but are also variety generators relative to the larger system they exist in. In our pursuit of our own interest, we are all agents of the second law. 

